{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0tb5CqVWpR2"
   },
   "source": [
    "# Part 2 Assignment: Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8djQAZudW0co"
   },
   "source": [
    "### Import Libraries & Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "t--A4pUQWlkq"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Hsazl7niW9gy"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('CE802_P2_Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uod0iQKxW5cI"
   },
   "source": [
    "## Data Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "YFrQMOCaXCcR",
    "outputId": "9aaf238e-c433-46ab-c81b-989a972916ef"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>F9</th>\n",
       "      <th>F10</th>\n",
       "      <th>F11</th>\n",
       "      <th>F12</th>\n",
       "      <th>F13</th>\n",
       "      <th>F14</th>\n",
       "      <th>F15</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>6.03</td>\n",
       "      <td>-4.56</td>\n",
       "      <td>30.00</td>\n",
       "      <td>63.04</td>\n",
       "      <td>0.55</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>0.96</td>\n",
       "      <td>-1.67</td>\n",
       "      <td>1</td>\n",
       "      <td>-339.75</td>\n",
       "      <td>0.84</td>\n",
       "      <td>9.90</td>\n",
       "      <td>-2.22</td>\n",
       "      <td>-9.53</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>1.41</td>\n",
       "      <td>-4.24</td>\n",
       "      <td>31.38</td>\n",
       "      <td>100.04</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.60</td>\n",
       "      <td>-3.67</td>\n",
       "      <td>1</td>\n",
       "      <td>-246.75</td>\n",
       "      <td>-0.62</td>\n",
       "      <td>12.32</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>630</td>\n",
       "      <td>43.35</td>\n",
       "      <td>0.50</td>\n",
       "      <td>50.70</td>\n",
       "      <td>219.04</td>\n",
       "      <td>-3.46</td>\n",
       "      <td>10.08</td>\n",
       "      <td>5.65</td>\n",
       "      <td>-14.67</td>\n",
       "      <td>10</td>\n",
       "      <td>-129.75</td>\n",
       "      <td>-3.40</td>\n",
       "      <td>1.80</td>\n",
       "      <td>8.48</td>\n",
       "      <td>-10.29</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>2.79</td>\n",
       "      <td>-3.58</td>\n",
       "      <td>31.83</td>\n",
       "      <td>69.04</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-1.67</td>\n",
       "      <td>1</td>\n",
       "      <td>-276.75</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>13.32</td>\n",
       "      <td>1.36</td>\n",
       "      <td>-9.92</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>0.36</td>\n",
       "      <td>-6.62</td>\n",
       "      <td>32.97</td>\n",
       "      <td>75.04</td>\n",
       "      <td>2.43</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.48</td>\n",
       "      <td>-0.67</td>\n",
       "      <td>1</td>\n",
       "      <td>-294.75</td>\n",
       "      <td>3.14</td>\n",
       "      <td>10.80</td>\n",
       "      <td>-5.86</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    F1     F2    F3     F4      F5    F6     F7    F8     F9  F10     F11  \\\n",
       "0   16   6.03 -4.56  30.00   63.04  0.55  -0.78  0.96  -1.67    1 -339.75   \n",
       "1    6   1.41 -4.24  31.38  100.04  0.06   1.08  0.60  -3.67    1 -246.75   \n",
       "2  630  43.35  0.50  50.70  219.04 -3.46  10.08  5.65 -14.67   10 -129.75   \n",
       "3   30   2.79 -3.58  31.83   69.04 -0.40  -0.09  0.18  -1.67    1 -276.75   \n",
       "4   18   0.36 -6.62  32.97   75.04  2.43   0.24  0.48  -0.67    1 -294.75   \n",
       "\n",
       "    F12    F13   F14    F15  Class  \n",
       "0  0.84   9.90 -2.22  -9.53  False  \n",
       "1 -0.62  12.32 -0.56    NaN  False  \n",
       "2 -3.40   1.80  8.48 -10.29  False  \n",
       "3 -0.32  13.32  1.36  -9.92  False  \n",
       "4  3.14  10.80 -5.86    NaN   True  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7bOMm2VXNZz",
    "outputId": "e3874333-a518-448f-c089-059f19a96cb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1500 entries, 0 to 1499\n",
      "Data columns (total 16 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   F1      1500 non-null   int64  \n",
      " 1   F2      1500 non-null   float64\n",
      " 2   F3      1500 non-null   float64\n",
      " 3   F4      1500 non-null   float64\n",
      " 4   F5      1500 non-null   float64\n",
      " 5   F6      1500 non-null   float64\n",
      " 6   F7      1500 non-null   float64\n",
      " 7   F8      1500 non-null   float64\n",
      " 8   F9      1500 non-null   float64\n",
      " 9   F10     1500 non-null   int64  \n",
      " 10  F11     1500 non-null   float64\n",
      " 11  F12     1500 non-null   float64\n",
      " 12  F13     1500 non-null   float64\n",
      " 13  F14     1500 non-null   float64\n",
      " 14  F15     750 non-null    float64\n",
      " 15  Class   1500 non-null   bool   \n",
      "dtypes: bool(1), float64(13), int64(2)\n",
      "memory usage: 177.4 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKXC8jEoCo8J"
   },
   "source": [
    "\n",
    "*   All features are either 'float' or 'int'\n",
    "*   F15 contains 50% missing values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "tslHM0LyJQtN",
    "outputId": "0c36d994-1de7-44b3-bcdb-7870bc4c9c11"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>F9</th>\n",
       "      <th>F10</th>\n",
       "      <th>F11</th>\n",
       "      <th>F12</th>\n",
       "      <th>F13</th>\n",
       "      <th>F14</th>\n",
       "      <th>F15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.00000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>750.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>218.049333</td>\n",
       "      <td>14.90628</td>\n",
       "      <td>-0.829733</td>\n",
       "      <td>41.063440</td>\n",
       "      <td>133.485333</td>\n",
       "      <td>-1.592947</td>\n",
       "      <td>5.390220</td>\n",
       "      <td>3.854280</td>\n",
       "      <td>-7.308000</td>\n",
       "      <td>5.403333</td>\n",
       "      <td>-307.992000</td>\n",
       "      <td>0.244707</td>\n",
       "      <td>7.315067</td>\n",
       "      <td>3.542427</td>\n",
       "      <td>-10.463013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>234.669160</td>\n",
       "      <td>12.97563</td>\n",
       "      <td>5.090345</td>\n",
       "      <td>9.500727</td>\n",
       "      <td>71.416874</td>\n",
       "      <td>2.513850</td>\n",
       "      <td>7.662813</td>\n",
       "      <td>3.465276</td>\n",
       "      <td>7.159315</td>\n",
       "      <td>4.504907</td>\n",
       "      <td>120.565344</td>\n",
       "      <td>2.217929</td>\n",
       "      <td>4.982472</td>\n",
       "      <td>5.081288</td>\n",
       "      <td>1.005117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-17.560000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>54.040000</td>\n",
       "      <td>-6.070000</td>\n",
       "      <td>-19.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-24.670000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-609.750000</td>\n",
       "      <td>-4.480000</td>\n",
       "      <td>-1.700000</td>\n",
       "      <td>-14.520000</td>\n",
       "      <td>-12.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>3.72000</td>\n",
       "      <td>-4.600000</td>\n",
       "      <td>32.160000</td>\n",
       "      <td>78.040000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>-14.670000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-354.750000</td>\n",
       "      <td>-1.060000</td>\n",
       "      <td>2.560000</td>\n",
       "      <td>-0.220000</td>\n",
       "      <td>-11.210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>6.21000</td>\n",
       "      <td>-2.810000</td>\n",
       "      <td>33.240000</td>\n",
       "      <td>98.040000</td>\n",
       "      <td>-0.610000</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>1.020000</td>\n",
       "      <td>-3.670000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-291.750000</td>\n",
       "      <td>-0.060000</td>\n",
       "      <td>9.220000</td>\n",
       "      <td>1.580000</td>\n",
       "      <td>-10.470000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>390.000000</td>\n",
       "      <td>24.60000</td>\n",
       "      <td>4.080000</td>\n",
       "      <td>49.350000</td>\n",
       "      <td>194.040000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>12.877500</td>\n",
       "      <td>7.050000</td>\n",
       "      <td>-1.670000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>-252.750000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>10.985000</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>-9.742500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>780.000000</td>\n",
       "      <td>47.10000</td>\n",
       "      <td>8.300000</td>\n",
       "      <td>60.450000</td>\n",
       "      <td>299.040000</td>\n",
       "      <td>6.070000</td>\n",
       "      <td>18.810000</td>\n",
       "      <td>10.350000</td>\n",
       "      <td>1.330000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>35.250000</td>\n",
       "      <td>8.720000</td>\n",
       "      <td>22.780000</td>\n",
       "      <td>12.860000</td>\n",
       "      <td>-7.740000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                F1          F2           F3           F4           F5  \\\n",
       "count  1500.000000  1500.00000  1500.000000  1500.000000  1500.000000   \n",
       "mean    218.049333    14.90628    -0.829733    41.063440   133.485333   \n",
       "std     234.669160    12.97563     5.090345     9.500727    71.416874   \n",
       "min       0.000000     0.00000   -17.560000    30.000000    54.040000   \n",
       "25%      30.000000     3.72000    -4.600000    32.160000    78.040000   \n",
       "50%     100.000000     6.21000    -2.810000    33.240000    98.040000   \n",
       "75%     390.000000    24.60000     4.080000    49.350000   194.040000   \n",
       "max     780.000000    47.10000     8.300000    60.450000   299.040000   \n",
       "\n",
       "                F6           F7           F8           F9          F10  \\\n",
       "count  1500.000000  1500.000000  1500.000000  1500.000000  1500.000000   \n",
       "mean     -1.592947     5.390220     3.854280    -7.308000     5.403333   \n",
       "std       2.513850     7.662813     3.465276     7.159315     4.504907   \n",
       "min      -6.070000   -19.800000     0.000000   -24.670000     0.000000   \n",
       "25%      -4.000000    -0.300000     0.600000   -14.670000     1.000000   \n",
       "50%      -0.610000     2.400000     1.020000    -3.670000     1.000000   \n",
       "75%       0.350000    12.877500     7.050000    -1.670000    10.000000   \n",
       "max       6.070000    18.810000    10.350000     1.330000    10.000000   \n",
       "\n",
       "               F11          F12          F13          F14         F15  \n",
       "count  1500.000000  1500.000000  1500.000000  1500.000000  750.000000  \n",
       "mean   -307.992000     0.244707     7.315067     3.542427  -10.463013  \n",
       "std     120.565344     2.217929     4.982472     5.081288    1.005117  \n",
       "min    -609.750000    -4.480000    -1.700000   -14.520000  -12.820000  \n",
       "25%    -354.750000    -1.060000     2.560000    -0.220000  -11.210000  \n",
       "50%    -291.750000    -0.060000     9.220000     1.580000  -10.470000  \n",
       "75%    -252.750000     1.400000    10.985000     8.500000   -9.742500  \n",
       "max      35.250000     8.720000    22.780000    12.860000   -7.740000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcKRpCzBC7GX"
   },
   "source": [
    "*   Data has very different ranges so will need to be scaled\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "uO97biA6QVJt",
    "outputId": "da643da1-44ab-4361-fa51-977c2cd55dec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Class', ylabel='count'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x=\"Class\", data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7JFDKT4fwgO"
   },
   "source": [
    "## Data Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "xXymxmpiJW7z"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from sklearn.metrics import classification_report, accuracy_score, make_scorer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Xl4_QeeEbfxO"
   },
   "outputs": [],
   "source": [
    "X = df.drop(['Class'], axis=1)\n",
    "y = df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "73S6WMNugbsT"
   },
   "outputs": [],
   "source": [
    "# initiate scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# create dictionary of imputation methods\n",
    "imputation_methods = {\n",
    "                      \"const\": SimpleImputer(strategy='constant', fill_value = 0, missing_values=np.nan), \n",
    "                      \"mean\": SimpleImputer(strategy='mean', missing_values=np.nan), \n",
    "                      \"knn\": KNNImputer(missing_values=np.nan),\n",
    "                      \"iter\": IterativeImputer(missing_values=np.nan, random_state = 0) \n",
    "}\n",
    "\n",
    "# create cross validation splits using KFold\n",
    "inner_cv = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYUOc_uu4qiw"
   },
   "source": [
    "### Create Parameter Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "07m3SklB4upd"
   },
   "outputs": [],
   "source": [
    "# create paramater space per model\n",
    "param_grid_tree = {\n",
    "    'classifier__criterion': ['gini', 'entropy'],\n",
    "    'classifier__max_features' : [None, 'auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "param_grid_forest = {\n",
    "    'classifier__criterion': ['gini', 'entropy'],\n",
    "    'classifier__max_features' : ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "param_grid_reg = {\n",
    "    'classifier__solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "}\n",
    "\n",
    "param_grid_knn = {\n",
    "    'classifier__n_neighbors': [1,3,5,7,10,15,20],\n",
    "    'classifier__metric': ['euclidean', 'manhattan', 'minkowski'],\n",
    "    'classifier__weights' : ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "param_grid_svm = {\n",
    "    'classifier__kernel': ['poly', 'rbf', 'sigmoid'],\n",
    "    'classifier__C': [0.01, 1, 5, 10, 100]\n",
    "}\n",
    "\n",
    "# create dictionary of classifiers with the classifier and paramaters as value \n",
    "classifiers = {\n",
    "              \"tree\": (DecisionTreeClassifier(), param_grid_tree),\n",
    "              \"forest\": (RandomForestClassifier(), param_grid_forest),\n",
    "               \"reg\": (LogisticRegression(), param_grid_reg),\n",
    "               'knn': (KNeighborsClassifier(), param_grid_knn),\n",
    "               'svm': (SVC(), param_grid_svm)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "mU6-ccAv5TIS"
   },
   "outputs": [],
   "source": [
    "# empty dictionary to store each model pipeline and respective params as values\n",
    "models_params = {}\n",
    "\n",
    "# loop over to fill dictionary\n",
    "for classifier_name, classifier in classifiers.items():\n",
    "  for method_name, method in imputation_methods.items():\n",
    "    pipe = Pipeline(steps=[('imputer', method),\n",
    "                           ('scaler', scaler),\n",
    "                           ('classifier', classifier[0])])\n",
    "    models_params[f\"{classifier_name}_{method_name}\"] = (pipe, classifier[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "IqOBZdgOAK33"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, make_scorer\n",
    "\n",
    "# variables initiated for classification report\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "\n",
    "# define custom scoring function\n",
    "def classification_report_with_accuracy_score(y_true, y_pred):\n",
    "    originalclass.extend(y_true)\n",
    "    predictedclass.extend(y_pred)\n",
    "    return accuracy_score(y_true, y_pred) # return accuracy score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0GoKNyke6JCm"
   },
   "source": [
    "### Finding the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rhNul0Qk6RLe",
    "outputId": "d4b7dcc3-441e-4f7d-9dfc-6835b64fc490"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree_const\n",
      "Accuracy in the outer folds: ['0.81', '0.77', '0.79', '0.77', '0.81'].\n",
      "Average Error: 0.79\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.80      0.82      0.81       809\n",
      "        True       0.78      0.76      0.77       691\n",
      "\n",
      "    accuracy                           0.79      1500\n",
      "   macro avg       0.79      0.79      0.79      1500\n",
      "weighted avg       0.79      0.79      0.79      1500\n",
      "\n",
      "tree_mean\n",
      "Accuracy in the outer folds: ['0.79', '0.80', '0.82', '0.77', '0.78'].\n",
      "Average Error: 0.79\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.80      0.81      0.81       809\n",
      "        True       0.78      0.77      0.77       691\n",
      "\n",
      "    accuracy                           0.79      1500\n",
      "   macro avg       0.79      0.79      0.79      1500\n",
      "weighted avg       0.79      0.79      0.79      1500\n",
      "\n",
      "tree_knn\n",
      "Accuracy in the outer folds: ['0.75', '0.76', '0.75', '0.78', '0.76'].\n",
      "Average Error: 0.76\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.77      0.78      0.78       809\n",
      "        True       0.74      0.73      0.74       691\n",
      "\n",
      "    accuracy                           0.76      1500\n",
      "   macro avg       0.76      0.76      0.76      1500\n",
      "weighted avg       0.76      0.76      0.76      1500\n",
      "\n",
      "tree_iter\n",
      "Accuracy in the outer folds: ['0.81', '0.79', '0.82', '0.77', '0.79'].\n",
      "Average Error: 0.79\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.81      0.80      0.81       809\n",
      "        True       0.77      0.79      0.78       691\n",
      "\n",
      "    accuracy                           0.79      1500\n",
      "   macro avg       0.79      0.79      0.79      1500\n",
      "weighted avg       0.79      0.79      0.79      1500\n",
      "\n",
      "forest_const\n",
      "Accuracy in the outer folds: ['0.88', '0.83', '0.88', '0.86', '0.84'].\n",
      "Average Error: 0.86\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.86      0.89      0.87       809\n",
      "        True       0.86      0.82      0.84       691\n",
      "\n",
      "    accuracy                           0.86      1500\n",
      "   macro avg       0.86      0.86      0.86      1500\n",
      "weighted avg       0.86      0.86      0.86      1500\n",
      "\n",
      "forest_mean\n",
      "Accuracy in the outer folds: ['0.90', '0.84', '0.88', '0.87', '0.87'].\n",
      "Average Error: 0.87\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.88      0.89      0.88       809\n",
      "        True       0.87      0.86      0.86       691\n",
      "\n",
      "    accuracy                           0.87      1500\n",
      "   macro avg       0.87      0.87      0.87      1500\n",
      "weighted avg       0.87      0.87      0.87      1500\n",
      "\n",
      "forest_knn\n",
      "Accuracy in the outer folds: ['0.89', '0.82', '0.87', '0.87', '0.88'].\n",
      "Average Error: 0.86\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.86      0.89      0.88       809\n",
      "        True       0.87      0.83      0.85       691\n",
      "\n",
      "    accuracy                           0.86      1500\n",
      "   macro avg       0.86      0.86      0.86      1500\n",
      "weighted avg       0.86      0.86      0.86      1500\n",
      "\n",
      "forest_iter\n",
      "Accuracy in the outer folds: ['0.88', '0.86', '0.90', '0.91', '0.87'].\n",
      "Average Error: 0.88\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.89      0.89      0.89       809\n",
      "        True       0.87      0.87      0.87       691\n",
      "\n",
      "    accuracy                           0.88      1500\n",
      "   macro avg       0.88      0.88      0.88      1500\n",
      "weighted avg       0.88      0.88      0.88      1500\n",
      "\n",
      "reg_const\n",
      "Accuracy in the outer folds: ['0.83', '0.80', '0.83', '0.83', '0.76'].\n",
      "Average Error: 0.81\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.84      0.80      0.82       809\n",
      "        True       0.78      0.82      0.80       691\n",
      "\n",
      "    accuracy                           0.81      1500\n",
      "   macro avg       0.81      0.81      0.81      1500\n",
      "weighted avg       0.81      0.81      0.81      1500\n",
      "\n",
      "reg_mean\n",
      "Accuracy in the outer folds: ['0.88', '0.83', '0.84', '0.83', '0.84'].\n",
      "Average Error: 0.84\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.85      0.87      0.86       809\n",
      "        True       0.84      0.82      0.83       691\n",
      "\n",
      "    accuracy                           0.84      1500\n",
      "   macro avg       0.84      0.84      0.84      1500\n",
      "weighted avg       0.84      0.84      0.84      1500\n",
      "\n",
      "reg_knn\n",
      "Accuracy in the outer folds: ['0.84', '0.82', '0.85', '0.80', '0.81'].\n",
      "Average Error: 0.82\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.84      0.83      0.84       809\n",
      "        True       0.81      0.81      0.81       691\n",
      "\n",
      "    accuracy                           0.82      1500\n",
      "   macro avg       0.82      0.82      0.82      1500\n",
      "weighted avg       0.82      0.82      0.82      1500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carte\\miniconda3\\envs\\ce889\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\carte\\miniconda3\\envs\\ce889\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\carte\\miniconda3\\envs\\ce889\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\carte\\miniconda3\\envs\\ce889\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\carte\\miniconda3\\envs\\ce889\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\carte\\miniconda3\\envs\\ce889\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\carte\\miniconda3\\envs\\ce889\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\carte\\miniconda3\\envs\\ce889\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\carte\\miniconda3\\envs\\ce889\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reg_iter\n",
      "Accuracy in the outer folds: ['0.89', '0.84', '0.86', '0.85', '0.85'].\n",
      "Average Error: 0.86\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.88      0.86      0.87       809\n",
      "        True       0.84      0.86      0.85       691\n",
      "\n",
      "    accuracy                           0.86      1500\n",
      "   macro avg       0.86      0.86      0.86      1500\n",
      "weighted avg       0.86      0.86      0.86      1500\n",
      "\n",
      "knn_const\n",
      "Accuracy in the outer folds: ['0.75', '0.75', '0.78', '0.75', '0.79'].\n",
      "Average Error: 0.76\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.76      0.83      0.79       809\n",
      "        True       0.78      0.68      0.73       691\n",
      "\n",
      "    accuracy                           0.76      1500\n",
      "   macro avg       0.77      0.76      0.76      1500\n",
      "weighted avg       0.76      0.76      0.76      1500\n",
      "\n",
      "knn_mean\n",
      "Accuracy in the outer folds: ['0.78', '0.80', '0.81', '0.81', '0.81'].\n",
      "Average Error: 0.80\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.78      0.87      0.82       809\n",
      "        True       0.82      0.72      0.77       691\n",
      "\n",
      "    accuracy                           0.80      1500\n",
      "   macro avg       0.80      0.79      0.80      1500\n",
      "weighted avg       0.80      0.80      0.80      1500\n",
      "\n",
      "knn_knn\n",
      "Accuracy in the outer folds: ['0.79', '0.78', '0.83', '0.79', '0.81'].\n",
      "Average Error: 0.80\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.79      0.86      0.82       809\n",
      "        True       0.82      0.73      0.77       691\n",
      "\n",
      "    accuracy                           0.80      1500\n",
      "   macro avg       0.80      0.79      0.80      1500\n",
      "weighted avg       0.80      0.80      0.80      1500\n",
      "\n",
      "knn_iter\n",
      "Accuracy in the outer folds: ['0.87', '0.82', '0.86', '0.88', '0.87'].\n",
      "Average Error: 0.86\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.84      0.90      0.87       809\n",
      "        True       0.88      0.81      0.84       691\n",
      "\n",
      "    accuracy                           0.86      1500\n",
      "   macro avg       0.86      0.85      0.86      1500\n",
      "weighted avg       0.86      0.86      0.86      1500\n",
      "\n",
      "svm_const\n",
      "Accuracy in the outer folds: ['0.89', '0.86', '0.89', '0.88', '0.86'].\n",
      "Average Error: 0.88\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.89      0.88      0.89       809\n",
      "        True       0.86      0.87      0.87       691\n",
      "\n",
      "    accuracy                           0.88      1500\n",
      "   macro avg       0.88      0.88      0.88      1500\n",
      "weighted avg       0.88      0.88      0.88      1500\n",
      "\n",
      "svm_mean\n",
      "Accuracy in the outer folds: ['0.90', '0.86', '0.89', '0.88', '0.88'].\n",
      "Average Error: 0.88\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.90      0.89      0.89       809\n",
      "        True       0.87      0.88      0.87       691\n",
      "\n",
      "    accuracy                           0.88      1500\n",
      "   macro avg       0.88      0.88      0.88      1500\n",
      "weighted avg       0.88      0.88      0.88      1500\n",
      "\n",
      "svm_knn\n",
      "Accuracy in the outer folds: ['0.90', '0.86', '0.89', '0.87', '0.90'].\n",
      "Average Error: 0.88\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.89      0.89      0.89       809\n",
      "        True       0.88      0.87      0.87       691\n",
      "\n",
      "    accuracy                           0.88      1500\n",
      "   macro avg       0.88      0.88      0.88      1500\n",
      "weighted avg       0.88      0.88      0.88      1500\n",
      "\n",
      "svm_iter\n",
      "Accuracy in the outer folds: ['0.91', '0.87', '0.89', '0.90', '0.88'].\n",
      "Average Error: 0.89\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.90      0.89      0.90       809\n",
      "        True       0.87      0.89      0.88       691\n",
      "\n",
      "    accuracy                           0.89      1500\n",
      "   macro avg       0.89      0.89      0.89      1500\n",
      "weighted avg       0.89      0.89      0.89      1500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_outer_score = dict()\n",
    "\n",
    "for name, (pipe, params) in models_params.items():\n",
    "    originalclass = []\n",
    "    predictedclass = []\n",
    "    # compute nested cross validation using GridSearchCV to find the optimal model for that pipeline\n",
    "    optimised_model = GridSearchCV(pipe, params, cv=inner_cv, scoring='accuracy')\n",
    "    scores = cross_val_score(optimised_model, X, y, cv=outer_cv, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "    # get the average of the outer fold scores\n",
    "    avg_outer_score[name] = np.mean(scores)\n",
    "    rounded_scores = [f\"{score:.2f}\" for score in scores] #rounded scores for print statement\n",
    "    print(f\"{name}\\nAccuracy in the outer folds: {rounded_scores}.\\nAverage Error: {np.mean(scores):.2f}\")\n",
    "    print()\n",
    "    print(classification_report(originalclass, predictedclass)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JiF4fCLZ_0Ld",
    "outputId": "24f91947-2a59-416b-c0e4-6274dc88835c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is:\n",
      "Pipeline(steps=[('imputer', IterativeImputer(random_state=0)),\n",
      "                ('scaler', StandardScaler()), ('classifier', SVC())]) \n",
      "With an average score of: 0.8886666666666667\n"
     ]
    }
   ],
   "source": [
    "# store best model, associated parameters and score\n",
    "best_model, best_model_score = max(avg_outer_score.items(),key=(lambda name_averagescore: name_averagescore[1]))\n",
    "best_model, best_model_params = models_params[name]\n",
    "\n",
    "print(f\"The best model is:\\n{best_model} \\nWith an average score of: {best_model_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OU7TVPoCybXG"
   },
   "source": [
    "## Applying to the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "KJf0LHN2yOD2"
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"CE802_P2_Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "NcZpn6vO4DEj"
   },
   "outputs": [],
   "source": [
    "test_inputs_df = test_df.drop(['Class'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "oFOX5HHTGe_t"
   },
   "outputs": [],
   "source": [
    "# fit our best model to the entire data set\n",
    "best_model.fit(X,y)\n",
    "y_pred = best_model.predict(test_inputs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tCW5IqcuGszd",
    "outputId": "8a91beb9-ebcc-479a-931c-8a9942c9e59e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    802\n",
       "True     698\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check value counts to see distribution\n",
    "y_pred = pd.Series(y_pred)\n",
    "y_pred.value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "ZFHQMwtkCh6Z",
    "outputId": "e3c58d38-9315-4798-b59d-d8e2335bc89e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>F9</th>\n",
       "      <th>F10</th>\n",
       "      <th>F11</th>\n",
       "      <th>F12</th>\n",
       "      <th>F13</th>\n",
       "      <th>F14</th>\n",
       "      <th>F15</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>5.58</td>\n",
       "      <td>-4.66</td>\n",
       "      <td>31.83</td>\n",
       "      <td>69.04</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>-1.29</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-3.67</td>\n",
       "      <td>1</td>\n",
       "      <td>-243.75</td>\n",
       "      <td>0.94</td>\n",
       "      <td>13.84</td>\n",
       "      <td>-1.48</td>\n",
       "      <td>-11.04</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80</td>\n",
       "      <td>37.95</td>\n",
       "      <td>4.40</td>\n",
       "      <td>50.70</td>\n",
       "      <td>199.04</td>\n",
       "      <td>-4.83</td>\n",
       "      <td>5.19</td>\n",
       "      <td>7.25</td>\n",
       "      <td>-4.67</td>\n",
       "      <td>10</td>\n",
       "      <td>-474.75</td>\n",
       "      <td>-3.34</td>\n",
       "      <td>0.46</td>\n",
       "      <td>8.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>1.08</td>\n",
       "      <td>-4.14</td>\n",
       "      <td>32.13</td>\n",
       "      <td>73.04</td>\n",
       "      <td>0.14</td>\n",
       "      <td>2.01</td>\n",
       "      <td>0.59</td>\n",
       "      <td>-3.67</td>\n",
       "      <td>1</td>\n",
       "      <td>-234.75</td>\n",
       "      <td>-1.08</td>\n",
       "      <td>9.36</td>\n",
       "      <td>-1.20</td>\n",
       "      <td>-11.71</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>240</td>\n",
       "      <td>34.95</td>\n",
       "      <td>3.74</td>\n",
       "      <td>44.85</td>\n",
       "      <td>264.04</td>\n",
       "      <td>-2.92</td>\n",
       "      <td>11.52</td>\n",
       "      <td>8.45</td>\n",
       "      <td>-14.67</td>\n",
       "      <td>10</td>\n",
       "      <td>-174.75</td>\n",
       "      <td>-3.20</td>\n",
       "      <td>2.94</td>\n",
       "      <td>4.14</td>\n",
       "      <td>-10.40</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42</td>\n",
       "      <td>4.11</td>\n",
       "      <td>-3.78</td>\n",
       "      <td>31.92</td>\n",
       "      <td>92.04</td>\n",
       "      <td>1.09</td>\n",
       "      <td>-2.67</td>\n",
       "      <td>0.72</td>\n",
       "      <td>-3.67</td>\n",
       "      <td>1</td>\n",
       "      <td>-282.75</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>11.20</td>\n",
       "      <td>0.92</td>\n",
       "      <td>-11.14</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>62</td>\n",
       "      <td>5.13</td>\n",
       "      <td>-5.32</td>\n",
       "      <td>32.46</td>\n",
       "      <td>72.04</td>\n",
       "      <td>1.17</td>\n",
       "      <td>-1.62</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1</td>\n",
       "      <td>-306.75</td>\n",
       "      <td>1.20</td>\n",
       "      <td>9.24</td>\n",
       "      <td>0.96</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>30</td>\n",
       "      <td>0.69</td>\n",
       "      <td>-3.96</td>\n",
       "      <td>32.70</td>\n",
       "      <td>78.04</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.57</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-1.67</td>\n",
       "      <td>1</td>\n",
       "      <td>-288.75</td>\n",
       "      <td>1.48</td>\n",
       "      <td>9.68</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>94</td>\n",
       "      <td>4.95</td>\n",
       "      <td>-5.38</td>\n",
       "      <td>32.19</td>\n",
       "      <td>91.04</td>\n",
       "      <td>1.99</td>\n",
       "      <td>1.47</td>\n",
       "      <td>0.56</td>\n",
       "      <td>-1.67</td>\n",
       "      <td>1</td>\n",
       "      <td>-252.75</td>\n",
       "      <td>-0.86</td>\n",
       "      <td>12.04</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>70</td>\n",
       "      <td>3.72</td>\n",
       "      <td>-6.82</td>\n",
       "      <td>33.06</td>\n",
       "      <td>74.04</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2.52</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1</td>\n",
       "      <td>-351.75</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>9.48</td>\n",
       "      <td>0.34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>52</td>\n",
       "      <td>3.09</td>\n",
       "      <td>-10.72</td>\n",
       "      <td>30.69</td>\n",
       "      <td>74.04</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.40</td>\n",
       "      <td>-1.67</td>\n",
       "      <td>0</td>\n",
       "      <td>-252.75</td>\n",
       "      <td>6.34</td>\n",
       "      <td>10.02</td>\n",
       "      <td>0.32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       F1     F2     F3     F4      F5    F6     F7    F8     F9  F10     F11  \\\n",
       "0       4   5.58  -4.66  31.83   69.04 -0.35  -1.29  0.06  -3.67    1 -243.75   \n",
       "1      80  37.95   4.40  50.70  199.04 -4.83   5.19  7.25  -4.67   10 -474.75   \n",
       "2      60   1.08  -4.14  32.13   73.04  0.14   2.01  0.59  -3.67    1 -234.75   \n",
       "3     240  34.95   3.74  44.85  264.04 -2.92  11.52  8.45 -14.67   10 -174.75   \n",
       "4      42   4.11  -3.78  31.92   92.04  1.09  -2.67  0.72  -3.67    1 -282.75   \n",
       "...   ...    ...    ...    ...     ...   ...    ...   ...    ...  ...     ...   \n",
       "1495   62   5.13  -5.32  32.46   72.04  1.17  -1.62  0.41   0.33    1 -306.75   \n",
       "1496   30   0.69  -3.96  32.70   78.04 -0.16  -0.57  0.01  -1.67    1 -288.75   \n",
       "1497   94   4.95  -5.38  32.19   91.04  1.99   1.47  0.56  -1.67    1 -252.75   \n",
       "1498   70   3.72  -6.82  33.06   74.04  0.50   2.52  0.24   0.33    1 -351.75   \n",
       "1499   52   3.09 -10.72  30.69   74.04  0.82   0.33  0.40  -1.67    0 -252.75   \n",
       "\n",
       "       F12    F13   F14    F15  Class  \n",
       "0     0.94  13.84 -1.48 -11.04   True  \n",
       "1    -3.34   0.46  8.72    NaN   True  \n",
       "2    -1.08   9.36 -1.20 -11.71   True  \n",
       "3    -3.20   2.94  4.14 -10.40  False  \n",
       "4    -0.40  11.20  0.92 -11.14   True  \n",
       "...    ...    ...   ...    ...    ...  \n",
       "1495  1.20   9.24  0.96    NaN   True  \n",
       "1496  1.48   9.68 -0.08    NaN   True  \n",
       "1497 -0.86  12.04 -0.28    NaN   True  \n",
       "1498 -0.08   9.48  0.34    NaN   True  \n",
       "1499  6.34  10.02  0.32    NaN   True  \n",
       "\n",
       "[1500 rows x 16 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add our predictions back to test dataframe\n",
    "test_df['Class'] = y_pred\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ZNX0jmNwKcXx"
   },
   "outputs": [],
   "source": [
    "# export to csv\n",
    "test_df.to_csv('CE802_P2_Results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Part_2_Assignment_Final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
